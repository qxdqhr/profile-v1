import { getAudioGenerator } from './audioGenerator';

interface Note {
  frequency: number;
  duration: number;
  volume?: number;
}

interface ChordProgression {
  name: string;
  chords: number[][];
  tempo: number;
  timeSignature: {
    numerator: number;
    denominator: number;
  };
}

interface MusicGenerationConfig {
  bpm: number;
  chordProgression: 'happy' | 'sad' | 'energetic' | 'peaceful' | 'custom';
  customChords?: number[][];
  timeSignature: {
    numerator: number;
    denominator: number;
  };
  duration: number; // ç§’
  volume: number;
  waveType: OscillatorType;
  enableHarmony: boolean;
  bassline: boolean;
}

export class MusicGenerator {
  private audioContext: AudioContext | null = null;
  private currentSource: AudioBufferSourceNode | null = null;
  private gainNode: GainNode | null = null;

  constructor() {
    this.initialize();
  }

  initialize() {
    if (typeof window !== 'undefined') {
      // If context doesn't exist or is closed, create a new one.
      if (!this.audioContext || this.audioContext.state === 'closed') {
        try {
          this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
          console.log('ğŸµ [MusicGenerator] AudioContext initialized/re-initialized. State:', this.audioContext.state);
          return true;
        } catch (e) {
          console.error('âŒ Web Audio API is not supported in this browser.', e);
          return false;
        }
      }
    }
    return !!this.audioContext;
  }

  async ensureContext() {
    if (!this.audioContext || this.audioContext.state === 'closed') {
      console.log('âš ï¸ [MusicGenerator] AudioContext is closed or not initialized. Re-initializing...');
      this.initialize();
    }
    if (this.audioContext && this.audioContext.state === 'suspended') {
      console.log('âš ï¸ [MusicGenerator] AudioContext is suspended. Resuming...');
      await this.audioContext.resume();
    }
  }
  
  getAudioContext(): AudioContext | null {
    return this.audioContext;
  }

  // éŸ³ç¬¦é¢‘ç‡å¯¹ç…§è¡¨
  static readonly NOTE_FREQUENCIES = {
    'C': [261.63, 523.25, 1046.50],
    'C#': [277.18, 554.37, 1108.73],
    'D': [293.66, 587.33, 1174.66],
    'D#': [311.13, 622.25, 1244.51],
    'E': [329.63, 659.25, 1318.51],
    'F': [349.23, 698.46, 1396.91],
    'F#': [369.99, 739.99, 1479.98],
    'G': [392.00, 783.99, 1567.98],
    'G#': [415.30, 830.61, 1661.22],
    'A': [440.00, 880.00, 1760.00],
    'A#': [466.16, 932.33, 1864.66],
    'B': [493.88, 987.77, 1975.53]
  };

  // é¢„å®šä¹‰çš„å’Œå¼¦è¿›è¡Œ
  static readonly CHORD_PROGRESSIONS: Record<string, ChordProgression> = {
    HAPPY_C: {
      name: 'æ¬¢ä¹Cå¤§è°ƒ',
      chords: [
        [261.63, 329.63, 392.00], // Cå¤§ä¸‰å’Œå¼¦
        [349.23, 440.00, 523.25], // Få¤§ä¸‰å’Œå¼¦
        [392.00, 493.88, 587.33], // Gå¤§ä¸‰å’Œå¼¦
        [261.63, 329.63, 392.00], // Cå¤§ä¸‰å’Œå¼¦
      ],
      tempo: 120,
      timeSignature: { numerator: 4, denominator: 4 }
    },
    SAD_A_MINOR: {
      name: 'å¿§éƒAå°è°ƒ',
      chords: [
        [220.00, 261.63, 329.63], // Aå°ä¸‰å’Œå¼¦
        [293.66, 349.23, 440.00], // Då°ä¸‰å’Œå¼¦
        [392.00, 466.16, 587.33], // Gå¤§ä¸‰å’Œå¼¦
        [220.00, 261.63, 329.63], // Aå°ä¸‰å’Œå¼¦
      ],
      tempo: 80,
      timeSignature: { numerator: 4, denominator: 4 }
    },
    ENERGETIC_E: {
      name: 'æ´»åŠ›Eå¤§è°ƒ',
      chords: [
        [329.63, 415.30, 493.88], // Eå¤§ä¸‰å’Œå¼¦
        [220.00, 277.18, 329.63], // Aå¤§ä¸‰å’Œå¼¦
        [493.88, 622.25, 739.99], // Bå¤§ä¸‰å’Œå¼¦
        [329.63, 415.30, 493.88], // Eå¤§ä¸‰å’Œå¼¦
      ],
      tempo: 140,
      timeSignature: { numerator: 4, denominator: 4 }
    },
    PEACEFUL_G: {
      name: 'å¹³å’ŒGå¤§è°ƒ',
      chords: [
        [392.00, 493.88, 587.33], // Gå¤§ä¸‰å’Œå¼¦
        [261.63, 329.63, 392.00], // Cå¤§ä¸‰å’Œå¼¦
        [293.66, 369.99, 440.00], // Då¤§ä¸‰å’Œå¼¦
        [392.00, 493.88, 587.33], // Gå¤§ä¸‰å’Œå¼¦
      ],
      tempo: 70,
      timeSignature: { numerator: 3, denominator: 4 }
    }
  };

  // ä¸»è¦çš„éŸ³ä¹ç”Ÿæˆæ–¹æ³• - æ”¯æŒé…ç½®å¯¹è±¡
  async generateMusic(config: Partial<MusicGenerationConfig>): Promise<AudioBuffer>;
  // é‡è½½ - æ”¯æŒç®€å•çš„BPMå‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
  async generateMusic(bpm: number): Promise<AudioBuffer>;
  // å®ç°
  async generateMusic(configOrBpm: Partial<MusicGenerationConfig> | number): Promise<AudioBuffer> {
    if (!this.initialize()) {
      throw new Error('AudioContext not initialized');
    }

    let config: MusicGenerationConfig;
    
    if (typeof configOrBpm === 'number') {
      // å‘åå…¼å®¹çš„ç®€å•ç‰ˆæœ¬
      config = {
        bpm: configOrBpm,
        chordProgression: 'happy',
        timeSignature: { numerator: 4, denominator: 4 },
        duration: 8,
        volume: 0.5,
        waveType: 'sine',
        enableHarmony: true,
        bassline: false
      };
    } else {
      // å®Œæ•´é…ç½®ç‰ˆæœ¬
      const defaultConfig: MusicGenerationConfig = {
        bpm: 120,
        chordProgression: 'happy',
        timeSignature: { numerator: 4, denominator: 4 },
        duration: 8,
        volume: 0.5,
        waveType: 'sine',
        enableHarmony: true,
        bassline: false
      };
      config = { ...defaultConfig, ...configOrBpm };
    }
    
    // é€‰æ‹©å’Œå¼¦è¿›è¡Œ
    let progression: ChordProgression;
    switch (config.chordProgression) {
      case 'sad':
        progression = MusicGenerator.CHORD_PROGRESSIONS.SAD_A_MINOR;
        break;
      case 'energetic':
        progression = MusicGenerator.CHORD_PROGRESSIONS.ENERGETIC_E;
        break;
      case 'peaceful':
        progression = MusicGenerator.CHORD_PROGRESSIONS.PEACEFUL_G;
        break;
      case 'custom':
        progression = {
          name: 'è‡ªå®šä¹‰',
          chords: config.customChords || MusicGenerator.CHORD_PROGRESSIONS.HAPPY_C.chords,
          tempo: config.bpm,
          timeSignature: config.timeSignature
        };
        break;
      default:
        progression = MusicGenerator.CHORD_PROGRESSIONS.HAPPY_C;
    }

    // æ›´æ–°å’Œå¼¦è¿›è¡Œçš„é…ç½®
    progression.tempo = config.bpm;
    progression.timeSignature = config.timeSignature;

    return this.generateChordProgression(progression, config);
  }

  private async generateChordProgression(
    progression: ChordProgression, 
    config: MusicGenerationConfig
  ): Promise<AudioBuffer> {
    const sampleRate = this.audioContext!.sampleRate;
    const secondsPerBeat = 60 / progression.tempo;
    const beatsPerMeasure = progression.timeSignature.numerator;
    const totalMeasures = Math.ceil(config.duration / (secondsPerBeat * beatsPerMeasure));
    const totalDuration = totalMeasures * beatsPerMeasure * secondsPerBeat;
    
    const numberOfChannels = 2; // ç«‹ä½“å£°
    const length = sampleRate * totalDuration;
    const buffer = this.audioContext!.createBuffer(numberOfChannels, length, sampleRate);
    
    const leftChannel = buffer.getChannelData(0);
    const rightChannel = buffer.getChannelData(1);

    // ç”ŸæˆéŸ³ä¹
    const chordsPerMeasure = Math.min(progression.chords.length, beatsPerMeasure);
    const chordDuration = secondsPerBeat * (beatsPerMeasure / chordsPerMeasure);

    for (let measure = 0; measure < totalMeasures; measure++) {
      for (let chordIndex = 0; chordIndex < chordsPerMeasure; chordIndex++) {
        const chord = progression.chords[chordIndex % progression.chords.length];
        const startTime = (measure * beatsPerMeasure + chordIndex * (beatsPerMeasure / chordsPerMeasure)) * secondsPerBeat;
        const startSample = Math.floor(startTime * sampleRate);
        const endSample = Math.floor((startTime + chordDuration) * sampleRate);

        // ç”Ÿæˆå’Œå¼¦
        this.generateChordInBuffer(
          leftChannel, 
          rightChannel, 
          chord, 
          startSample, 
          endSample, 
          config,
          chordDuration
        );

        // æ·»åŠ ä½éŸ³çº¿
        if (config.bassline) {
          this.generateBasslineInBuffer(
            leftChannel,
            rightChannel,
            chord[0] / 2, // ä½å…«åº¦
            startSample,
            endSample,
            config,
            chordDuration
          );
        }
      }
    }

    return buffer;
  }

  private generateChordInBuffer(
    leftChannel: Float32Array,
    rightChannel: Float32Array,
    chord: number[],
    startSample: number,
    endSample: number,
    config: MusicGenerationConfig,
    duration: number
  ) {
    const fadeTime = 0.05; // 50msæ·¡å…¥æ·¡å‡º
    const fadeSamples = fadeTime * this.audioContext!.sampleRate;

    for (let i = startSample; i < endSample && i < leftChannel.length; i++) {
      let leftSample = 0;
      let rightSample = 0;
      
      const t = i / this.audioContext!.sampleRate;
      const chordProgress = i - startSample;
      
      // ç”Ÿæˆå’Œå¼¦éŸ³ç¬¦
      chord.forEach((frequency, noteIndex) => {
        let noteVolume = config.volume / chord.length;
        
        // ä¸ºä¸åŒéŸ³ç¬¦æ·»åŠ ç«‹ä½“å£°æ•ˆæœ
        const panLeft = 0.5 + (noteIndex - chord.length / 2) * 0.2;
        const panRight = 1 - panLeft;
        
        let sample = 0;
        switch (config.waveType) {
          case 'sine':
            sample = Math.sin(2 * Math.PI * frequency * t);
            break;
          case 'square':
            sample = Math.sign(Math.sin(2 * Math.PI * frequency * t));
            break;
          case 'sawtooth':
            sample = 2 * (t * frequency - Math.floor(t * frequency + 0.5));
            break;
          case 'triangle':
            sample = 2 * Math.abs(2 * (t * frequency - Math.floor(t * frequency + 0.5))) - 1;
            break;
        }
        
        leftSample += sample * noteVolume * panLeft;
        rightSample += sample * noteVolume * panRight;
      });
      
      // åº”ç”¨æ·¡å…¥æ·¡å‡º
      let amplitude = 1;
      if (chordProgress < fadeSamples) {
        amplitude = chordProgress / fadeSamples;
      } else if (i > endSample - fadeSamples) {
        amplitude = (endSample - i) / fadeSamples;
      }
      
      leftChannel[i] += leftSample * amplitude;
      rightChannel[i] += rightSample * amplitude;
    }
  }

  private generateBasslineInBuffer(
    leftChannel: Float32Array,
    rightChannel: Float32Array,
    frequency: number,
    startSample: number,
    endSample: number,
    config: MusicGenerationConfig,
    duration: number
  ) {
    const bassVolume = config.volume * 0.6; // ä½éŸ³ç¨å¾®å°ä¸€äº›
    
    for (let i = startSample; i < endSample && i < leftChannel.length; i++) {
      const t = i / this.audioContext!.sampleRate;
      const sample = Math.sin(2 * Math.PI * frequency * t) * bassVolume;
      
      // ä½éŸ³ä¸»è¦åœ¨å·¦å£°é“
      leftChannel[i] += sample * 0.8;
      rightChannel[i] += sample * 0.2;
    }
  }

  // ç”ŸæˆæŒ‡å®šé…ç½®çš„éŸ³ä¹
  async generateConfiguredMusic(config: MusicGenerationConfig): Promise<AudioBuffer> {
    return this.generateMusic(config);
  }

  // è·å–å¯ç”¨çš„å’Œå¼¦è¿›è¡Œåˆ—è¡¨
  static getAvailableProgressions(): Array<{key: string, name: string}> {
    return Object.entries(MusicGenerator.CHORD_PROGRESSIONS).map(([key, progression]) => ({
      key: key.toLowerCase().replace('_', '-'),
      name: progression.name
    }));
  }

  async playBuffer(buffer: AudioBuffer, volume: number = 1.0): Promise<void> {
    console.log('ğŸµ [MusicGenerator] å¼€å§‹æ’­æ”¾éŸ³é¢‘ç¼“å†²åŒº');
    await this.ensureContext(); // æ’­æ”¾å‰ç¡®ä¿ context å¯ç”¨

    if (!this.audioContext) {
      console.error('âŒ [MusicGenerator] AudioContext not available.');
      return;
    }
    console.log('ğŸµ [MusicGenerator] AudioContextçŠ¶æ€:', this.audioContext.state);

    this.stop(); // åœæ­¢ä»»ä½•æ­£åœ¨æ’­æ”¾çš„éŸ³é¢‘

    const source = this.audioContext.createBufferSource();
    this.currentSource = source;
    source.buffer = buffer;

    const gainNode = this.audioContext.createGain();
    this.gainNode = gainNode;
    gainNode.gain.value = volume;

    source.connect(gainNode);
    gainNode.connect(this.audioContext.destination);

    return new Promise((resolve, reject) => {
      source.onended = () => {
        console.log('ğŸµ [MusicGenerator] éŸ³é¢‘æ’­æ”¾ç»“æŸ');
        resolve();
      };
      try {
        console.log('ğŸµ [MusicGenerator] å¼€å§‹æ’­æ”¾éŸ³é¢‘æº');
        source.start(0);
        console.log('ğŸµ [MusicGenerator] éŸ³é¢‘æºæ’­æ”¾æˆåŠŸ');
        resolve();
      } catch (error) {
        console.error('âŒ [MusicGenerator] æ’­æ”¾éŸ³é¢‘ç¼“å†²åŒºå¤±è´¥:', error);
        reject(error);
      }
    });
  }

  stop() {
    if (this.currentSource) {
      this.currentSource.stop();
      this.currentSource.disconnect();
      this.currentSource = null;
    }
    if (this.gainNode) {
      this.gainNode.disconnect();
      this.gainNode = null;
    }
  }

  // å‘åå…¼å®¹çš„æ–¹æ³•
  async generateHappyBackgroundMusic(): Promise<AudioBuffer> {
    await getAudioGenerator().initialize();
    return this.generateMusic({ chordProgression: 'happy', duration: 8 });
  }
}

// å¯¼å‡ºå‡½æ•°ç”¨äºå‘åå…¼å®¹
export async function generateHappyBackgroundMusicBase64(): Promise<string> {
  const audioGenerator = getAudioGenerator();
  await audioGenerator.initialize();
  
  const musicGenerator = new MusicGenerator();
  const buffer = await musicGenerator.generateHappyBackgroundMusic();
  
  // è¿™é‡Œéœ€è¦å°†AudioBufferè½¬æ¢ä¸ºbase64ï¼Œä½†è¿™éœ€è¦æ›´å¤æ‚çš„å®ç°
  // æš‚æ—¶è¿”å›ä¸€ä¸ªå ä½ç¬¦
  return 'data:audio/wav;base64,placeholder';
} 